{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import uuid\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Loading all important files: The individual documents, the pairs and the golden data (with paraphrases)\"\"\"\n",
    "with open('./data/documents_en_nometadata.json','r') as f:\n",
    "    documents_all = json.load(f)\n",
    "with open('./data/pairs_en.json','r') as f:\n",
    "    pairs_all = json.load(f)\n",
    "with open('./data/golden-data-new.json','r') as f:\n",
    "    golden_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Check that no golden-data pair appears in the pair set \"\"\"\n",
    "gpairs_set = []\n",
    "for g in golden_data:\n",
    "    gpairs_set.append(set((g['reference']['id'],g['paraphrase']['id'])))\n",
    "    gpairs_set.append(set((g['reference']['id'],g['high']['id'])))\n",
    "    gpairs_set.append(set((g['reference']['id'],g['medium']['id'])))\n",
    "    gpairs_set.append(set((g['reference']['id'],g['low']['id'])))\n",
    "    gpairs_set.append(set((g['reference']['id'],g['none']['id'])))\n",
    "\n",
    "pairs_set = []\n",
    "for p in pairs_all:\n",
    "    pairs_set.append(set((p['id1'],p['id2'])))\n",
    "    \n",
    "assert(any([(p in gpairs_set) for p in pairs_set]) == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomPairs(counter, n):\n",
    "    p = [(6-c) for c in counter]\n",
    "    p = [pp/sum(p) for pp in p]\n",
    "    indexes = np.random.choice(a=list(range(len(pairs_all))),size=(1,3),replace=False,p=p)[0]\n",
    "    pairs = []\n",
    "    for i in indexes:\n",
    "        pair = pairs_all[i]\n",
    "        document1 = {\"id\": pair['id1']}\n",
    "        document1['body'] = [d['text'] for d in documents_all if d['id'] == document1['id']][0]\n",
    "        document2 = {\"id\": pair['id2']}\n",
    "        document2['body'] = [d['text'] for d in documents_all if d['id'] == document2['id']][0]\n",
    "        pair_p = {'document_1': document1, 'document_2': document2, 'g_id': 0}\n",
    "        pairs.append((pair_p,i))\n",
    "    return pairs\n",
    "\n",
    "def getRandomGoldenPairs():\n",
    "    relation_levels = ['none','low','medium','high']\n",
    "    [gset1, gset2] = random.sample(golden_data,2)\n",
    "    [level1, level2] = random.sample(list(range(4)),2)\n",
    "    \n",
    "    document1_1 = {\"id\": gset1['reference']['id'], 'body':gset1['reference']['text']}\n",
    "    document1_2 = {\"id\": gset1[relation_levels[level1]]['id'], 'body':gset1[relation_levels[level1]]['text']}\n",
    "    pair_1 = {\n",
    "        'document_1': document1_1,\n",
    "        'document_2': document1_2,\n",
    "        'g_id': (2 if level1>level2 else 1)\n",
    "    }\n",
    "    \n",
    "    document2_1 = {\"id\": gset2['reference']['id'], 'body':gset2['reference']['text']}\n",
    "    document2_2 = {\"id\": gset2[relation_levels[level2]]['id'], 'body':gset2[relation_levels[level2]]['text']}\n",
    "    pair_2 = {\n",
    "        'document_1': document2_1,\n",
    "        'document_2': document2_2,\n",
    "        'g_id': (2 if level2>level1 else 1)\n",
    "    }\n",
    "    return (pair_1, pair_2)\n",
    "\n",
    "def generateDocumentPairSet(counter, n=3):\n",
    "    (gpair1, gpair2) = getRandomGoldenPairs()\n",
    "    pairs = getRandomPairs(counter, n)\n",
    "    return [gpair1] + [p for (p,i) in pairs] + [gpair2], [i for (p,i) in pairs]\n",
    "\n",
    "def generateDocumentPairSets():\n",
    "    counter = [0]*len(pairs_all)\n",
    "    documentSets = []\n",
    "    while (any([c < 5 for c in counter])):\n",
    "        documentSet, indexes = generateDocumentPairSet(counter)\n",
    "        documentSets.append(documentSet)\n",
    "        for i in indexes:\n",
    "            counter[i] = counter[i] + 1\n",
    "    return documentSets, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203 [6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Generates the sets of pairs of documents. Each pair will go on one HIT. We make sure each pair appears in at least 5 hits.\"\"\"\n",
    "ds, c = generateDocumentPairSets()\n",
    "print(len(ds), c)\n",
    "\n",
    "# Uncomment this to overwrite\n",
    "#with open('./data/DocumentPairSetsForHITS.json','w+') as f:\n",
    "#    json.dump(ds,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomRanking(counter, target):\n",
    "    p = [max((target-c),0.01) for c in counter]\n",
    "    p = [pp/sum(p) for pp in p]\n",
    "    main_document = {\"id\": np.random.choice(a=pairs_all,p=p)['id1']}\n",
    "    main_document['body'] = [d['text'] for d in documents_all if d['id'] == main_document['id']][0]\n",
    "    \n",
    "    documents = []\n",
    "    indexes = []\n",
    "    pairs_to_main_document_indexes = [i for (i,pair) in enumerate(pairs_all) if pair['id1'] == main_document['id']]\n",
    "    p2 = [max((target-counter[i]),0.01) for i in pairs_to_main_document_indexes]\n",
    "    p2 = [pp/sum(p2) for pp in p2]\n",
    "    for i in np.random.choice(a=pairs_to_main_document_indexes,size=(1,3),replace=False,p=p2)[0]:\n",
    "        documents.append({\n",
    "            'id': pairs_all[i]['id2'],\n",
    "            'body': [d['text'] for d in documents_all if d['id'] == pairs_all[i]['id2']][0],\n",
    "            'g_id': 0\n",
    "        })\n",
    "        indexes.append(i)\n",
    "        \n",
    "    return {'main_document': main_document, 'documents': documents}, indexes\n",
    "\n",
    "def getRandomGoldenRanking():\n",
    "    relation_levels = ['none','low','medium','high','paraphrase']\n",
    "    gset = random.choice(golden_data)\n",
    "    levels = [0,random.choice([1,2]),4]\n",
    "    \n",
    "    main_document = {\n",
    "        'id': gset['reference']['id'],\n",
    "        'body': gset['reference']['text']\n",
    "    }\n",
    "    \n",
    "    documents = []\n",
    "    for l in levels:\n",
    "        d = gset[relation_levels[l]]\n",
    "        g_id = 0\n",
    "        if (l==min(levels)):\n",
    "            g_id = 1\n",
    "        elif (l==max(levels)):\n",
    "            g_id = 2\n",
    "        documents.append({\n",
    "            'id': d['id'],\n",
    "            'body': d['text'],\n",
    "            'g_id': g_id\n",
    "        })\n",
    "        \n",
    "    return {'main_document': main_document, 'documents': documents}\n",
    "\n",
    "def generateDocumentRankingSet(counter, target, n=4):\n",
    "    granking = getRandomGoldenRanking()\n",
    "    rankings = []\n",
    "    while True:\n",
    "        ranking = getRandomRanking(counter=counter, target=target)\n",
    "        if any([ranking[0]['main_document']['id'] == r[0]['main_document']['id'] for r in rankings]):\n",
    "            continue\n",
    "        else:\n",
    "            rankings.append(ranking)\n",
    "            if len(rankings) == n:\n",
    "                break\n",
    "    return [granking] + [r for (r,i) in rankings], [i for (r,i) in rankings]\n",
    "\n",
    "def generateDocumentRankingSets():\n",
    "    target=25\n",
    "    counter = [0]*len(pairs_all)\n",
    "    documentSets = []\n",
    "    while (any([c < target for c in counter])):\n",
    "        documentSet, indexes = generateDocumentRankingSet(counter=counter, target=target)\n",
    "        documentSets.append({'_id':uuid.uuid4(),'documents':documentSet})\n",
    "        for i in indexes:\n",
    "            for ii in i:\n",
    "                counter[ii] = counter[ii] + 1\n",
    "    return documentSets, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 [25, 25, 25, 25, 26, 25, 25, 25, 25, 25, 25, 26, 25, 26, 25, 25, 25, 25, 25, 25, 25, 26, 25, 25, 26, 25, 26, 27, 26, 26, 26, 25, 26, 27, 25, 27, 27, 25, 26, 25, 27, 25, 27, 26, 26, 25, 27, 26, 27, 25, 25, 28, 27, 25, 26, 27, 26, 25, 25, 25, 26, 26, 25, 25, 25, 26, 25, 25, 25, 25, 26, 25, 25, 25, 25, 28, 26, 27, 26, 27, 28, 27, 30, 26, 27, 27, 27, 28, 25, 27, 25, 26, 25, 25, 26, 25, 26, 25, 25, 25, 26, 25]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Same as with the pairs. We make sure each ranking appears at least in a target amount of HITs (default is 25, see above)\"\"\"\n",
    "ds, c = generateDocumentRankingSets()\n",
    "print(len(ds), c)\n",
    "\n",
    "# Uncomment this to overwrite\n",
    "#with open('./data/DocumentRankingSetsForHITS.json','w+') as f:\n",
    "#    json.dump(ds,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
